Index: exp/exp_informer.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from data.data_loader import Dataset_ETT_hour, Dataset_ETT_minute, Dataset_Custom, Dataset_Pred\r\nfrom exp.exp_basic import Exp_Basic\r\nfrom models.model import Informer, InformerStack\r\n\r\nfrom utils.tools import EarlyStopping, adjust_learning_rate\r\nfrom utils.metrics import metric\r\n\r\nimport numpy as np\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch import optim\r\nfrom torch.utils.data import DataLoader\r\n\r\nimport os\r\nimport time\r\n\r\nimport warnings\r\nwarnings.filterwarnings('ignore')\r\n\r\nclass Exp_Informer(Exp_Basic):\r\n    def __init__(self, args):\r\n        super(Exp_Informer, self).__init__(args)\r\n    \r\n    def _build_model(self):\r\n        model_dict = {\r\n            'informer':Informer,\r\n            'informerstack':InformerStack,\r\n        }\r\n        if self.args.model=='informer' or self.args.model=='informerstack':\r\n            e_layers = self.args.e_layers if self.args.model=='informer' else self.args.s_layers\r\n            model = model_dict[self.args.model](\r\n                self.args.enc_in,\r\n                self.args.dec_in, \r\n                self.args.c_out, \r\n                self.args.seq_len, \r\n                self.args.label_len,\r\n                self.args.pred_len, \r\n                self.args.factor,\r\n                self.args.d_model, \r\n                self.args.n_heads, \r\n                e_layers, # self.args.e_layers,\r\n                self.args.d_layers, \r\n                self.args.d_ff,\r\n                self.args.dropout, \r\n                self.args.attn,\r\n                self.args.embed,\r\n                self.args.freq,\r\n                self.args.activation,\r\n                self.args.output_attention,\r\n                self.args.distil,\r\n                self.args.mix,\r\n                self.device\r\n            ).float()\r\n        \r\n        if self.args.use_multi_gpu and self.args.use_gpu:\r\n            model = nn.DataParallel(model, device_ids=self.args.device_ids)\r\n        return model\r\n\r\n    def _get_data(self, flag):\r\n        args = self.args\r\n\r\n        data_dict = {\r\n            'ETTh1':Dataset_ETT_hour,\r\n            'ETTh2':Dataset_ETT_hour,\r\n            'ETTm1':Dataset_ETT_minute,\r\n            'ETTm2':Dataset_ETT_minute,\r\n            'WTH':Dataset_Custom,\r\n            'ECL':Dataset_Custom,\r\n            'Solar':Dataset_Custom,\r\n            'custom':Dataset_Custom,\r\n        }\r\n\r\n        # 增加代码：23/4/25 yc 如果数据不存在于data_dict中，则往其中添加\r\n        if args.data not in data_dict.keys():\r\n            data_dict.update({args.data: \"Dataset_Custom\"})\r\n\r\n        Data = data_dict[self.args.data]\r\n        timeenc = 0 if args.embed!='timeF' else 1\r\n\r\n        if flag == 'test':\r\n            shuffle_flag = False; drop_last = True; batch_size = args.batch_size; freq=args.freq\r\n        elif flag=='pred':\r\n            shuffle_flag = False; drop_last = False; batch_size = 1; freq=args.detail_freq\r\n            Data = Dataset_Pred\r\n        else:\r\n            shuffle_flag = True; drop_last = True; batch_size = args.batch_size; freq=args.freq\r\n        data_set = Data(\r\n            root_path=args.root_path,\r\n            data_path=args.data_path,\r\n            flag=flag,\r\n            size=[args.seq_len, args.label_len, args.pred_len],\r\n            features=args.features,\r\n            target=args.target,\r\n            inverse=args.inverse,\r\n            timeenc=timeenc,\r\n            freq=freq,\r\n            cols=args.cols\r\n        )\r\n        print(flag, len(data_set))\r\n        data_loader = DataLoader(\r\n            data_set,\r\n            batch_size=batch_size,\r\n            shuffle=shuffle_flag,\r\n            num_workers=args.num_workers,\r\n            drop_last=drop_last)\r\n\r\n        return data_set, data_loader\r\n\r\n    def _select_optimizer(self):\r\n        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\r\n        return model_optim\r\n    \r\n    def _select_criterion(self):\r\n        criterion =  nn.MSELoss()\r\n        return criterion\r\n\r\n    def vali(self, setting, vali_data, vali_loader, criterion):\r\n        self.model.eval()\r\n\r\n        # 增加代码 23/4/25 yc\r\n        preds_vali=[]\r\n        trues_vali=[]\r\n\r\n        total_loss = []\r\n        # 23/4/25 16:13增加代码\r\n        log = open(fr'total_loss.csv', mode=\"a+\", encoding=\"utf-8\")  # 增加代码\r\n        for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(vali_loader):\r\n            pred, true = self._process_one_batch(\r\n                vali_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\r\n            # 增加代码 23/4/25\r\n            preds_vali.append(pred.detach().cpu().numpy())\r\n            trues_vali.append(true.detach().cpu().numpy())\r\n\r\n            loss = criterion(pred.detach().cpu(), true.detach().cpu())\r\n            # 增加代码同上时间\r\n            print(\"iters,{0},loss,{1:.7f}\".format(i+1,loss), file=log)\r\n            total_loss.append(loss)\r\n        # 增加代码  23/4/25\r\n        print(\"test end\",file=log)\r\n\r\n        # 23/4/25增加代码同上时间\r\n        preds_vali = np.array(preds_vali)\r\n        trues_vali = np.array(trues_vali)\r\n        print('test shape:', preds_vali.shape, trues_vali.shape)\r\n        preds_vali = preds_vali.reshape(-1, preds_vali.shape[-2], preds_vali.shape[-1])\r\n        trues_vali = trues_vali.reshape(-1, trues_vali.shape[-2], trues_vali.shape[-1])\r\n        print('test shape:', preds_vali.shape, trues_vali.shape)\r\n\r\n        folder_path = './vali_results/' + setting +'/'\r\n        if not os.path.exists(folder_path):\r\n            os.makedirs(folder_path)\r\n\r\n        mae, mse, rmse, mape, mspe = metric(preds_vali, trues_vali)\r\n\r\n        np.save(folder_path+'vali_metrics.npy', np.array([mae, mse, rmse, mape, mspe]))\r\n        np.save(folder_path+'vali_pred.npy', preds_vali)\r\n        np.save(folder_path+'vali_true.npy', trues_vali)\r\n\r\n        log.close()\r\n        total_loss = np.average(total_loss)\r\n        self.model.train()\r\n        return total_loss\r\n\r\n    def train(self, setting):\r\n        train_data, train_loader = self._get_data(flag = 'train')\r\n        vali_data, vali_loader = self._get_data(flag = 'val')\r\n        test_data, test_loader = self._get_data(flag = 'test')\r\n\r\n        path = os.path.join(self.args.checkpoints, setting)\r\n        if not os.path.exists(path):\r\n            os.makedirs(path)\r\n\r\n        time_now = time.time()\r\n        \r\n        train_steps = len(train_loader)\r\n        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\r\n        \r\n        model_optim = self._select_optimizer()\r\n        criterion = self._select_criterion()\r\n\r\n        if self.args.use_amp:\r\n            scaler = torch.cuda.amp.GradScaler()\r\n\r\n        # 增加代码\r\n        # 训练的时候记录每个epoch产生的损失，包括训练集损失、验证集损失、测试集(评估集)损失\r\n        all_epoch_train_loss = []\r\n        all_epoch_vali_loss = []\r\n        all_epoch_test_loss = []\r\n\r\n        # 增加代码\r\n        # 训练args.train_epochs个epoch，每一个epoch循环一遍整个数据集\r\n        epoch_count = 0\r\n\r\n        # 增加代码\r\n        log = open(fr'evry_loss.csv',mode=\"a+\",encoding=\"utf-8\")\r\n        for epoch in range(self.args.train_epochs):\r\n            iter_count = 0\r\n            train_loss = []\r\n            \r\n            self.model.train()\r\n            epoch_time = time.time()\r\n            for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(train_loader):\r\n                iter_count += 1\r\n                \r\n                model_optim.zero_grad()\r\n                pred, true = self._process_one_batch(\r\n                    train_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\r\n                loss = criterion(pred, true)\r\n                train_loss.append(loss.item())\r\n                \r\n                if (i+1) % 100==0:\r\n                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\r\n                    speed = (time.time()-time_now)/iter_count\r\n                    left_time = speed*((self.args.train_epochs - epoch)*train_steps - i)\r\n                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\r\n                    iter_count = 0\r\n                    time_now = time.time()\r\n                \r\n                if self.args.use_amp:\r\n                    scaler.scale(loss).backward()\r\n                    scaler.step(model_optim)\r\n                    scaler.update()\r\n                else:\r\n                    loss.backward()\r\n                    model_optim.step()\r\n\r\n            print(\"Epoch: {} cost time: {}\".format(epoch+1, time.time()-epoch_time))\r\n            train_loss = np.average(train_loss)\r\n            vali_loss = self.vali(setting,vali_data, vali_loader, criterion)\r\n            test_loss = self.vali(setting,test_data, test_loader, criterion)\r\n\r\n            # 添加代码23/4/25 添加到列表中留存，其中round()表示四舍五入\r\n            all_epoch_train_loss.append(float(round(train_loss, 3)))\r\n            all_epoch_vali_loss.append(float(round(vali_loss, 3)))\r\n            all_epoch_test_loss.append(float(round(test_loss, 3)))\r\n\r\n            # 完成每个epoch的训练就打印一次\r\n            # 增加代码\r\n            print(\"Epoch: {0},  Train Loss, {1:.7f}, Vali Loss, {2:.7f}, Test Loss, {3:.7f}\".format(\r\n                epoch + 1, train_loss, vali_loss, test_loss),file=log)\r\n            print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\r\n                epoch + 1, train_steps, train_loss, vali_loss, test_loss))\r\n            early_stopping(vali_loss, self.model, path)\r\n            if early_stopping.early_stop:\r\n                print(\"Early stopping\")\r\n                break\r\n\r\n            adjust_learning_rate(model_optim, epoch+1, self.args)\r\n        # 增加代码23/4/25\r\n        log.close()\r\n        best_model_path = path+'/'+'checkpoint.pth'\r\n        self.model.load_state_dict(torch.load(best_model_path))\r\n        \r\n        return self.model\r\n\r\n    def test(self, setting):\r\n        test_data, test_loader = self._get_data(flag='test')\r\n        \r\n        self.model.eval()\r\n        \r\n        preds = []\r\n        trues = []\r\n        \r\n        for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(test_loader):\r\n            pred, true = self._process_one_batch(\r\n                test_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\r\n            preds.append(pred.detach().cpu().numpy())\r\n            trues.append(true.detach().cpu().numpy())\r\n\r\n        preds = np.array(preds)\r\n        trues = np.array(trues)\r\n        print('test shape:', preds.shape, trues.shape)\r\n        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\r\n        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\r\n        print('test shape:', preds.shape, trues.shape)\r\n\r\n        # result save\r\n        folder_path = './results/' + setting +'/'\r\n        if not os.path.exists(folder_path):\r\n            os.makedirs(folder_path)\r\n\r\n        mae, mse, rmse, mape, mspe = metric(preds, trues)\r\n        print('mse:{}, mae:{}'.format(mse, mae))\r\n\r\n        np.save(folder_path+'metrics.npy', np.array([mae, mse, rmse, mape, mspe]))\r\n        np.save(folder_path+'pred.npy', preds)\r\n        np.save(folder_path+'true.npy', trues)\r\n\r\n        return\r\n\r\n    def predict(self, setting, load=False):\r\n        pred_data, pred_loader = self._get_data(flag='pred')\r\n        \r\n        if load:\r\n            path = os.path.join(self.args.checkpoints, setting)\r\n            best_model_path = path+'/'+'checkpoint.pth'\r\n            self.model.load_state_dict(torch.load(best_model_path))\r\n\r\n        self.model.eval()\r\n        \r\n        preds = []\r\n        \r\n        for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(pred_loader):\r\n            pred, true = self._process_one_batch(\r\n                pred_data, batch_x, batch_y, batch_x_mark, batch_y_mark)\r\n            preds.append(pred.detach().cpu().numpy())\r\n\r\n        preds = np.array(preds)\r\n        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\r\n        \r\n        # result save\r\n        folder_path = './results/' + setting +'/'\r\n        if not os.path.exists(folder_path):\r\n            os.makedirs(folder_path)\r\n        \r\n        np.save(folder_path+'real_prediction.npy', preds)\r\n        \r\n        return\r\n\r\n    def _process_one_batch(self, dataset_object, batch_x, batch_y, batch_x_mark, batch_y_mark):\r\n        batch_x = batch_x.float().to(self.device)\r\n        batch_y = batch_y.float()\r\n\r\n        batch_x_mark = batch_x_mark.float().to(self.device)\r\n        batch_y_mark = batch_y_mark.float().to(self.device)\r\n\r\n        # decoder input\r\n        if self.args.padding==0:\r\n            dec_inp = torch.zeros([batch_y.shape[0], self.args.pred_len, batch_y.shape[-1]]).float()\r\n        elif self.args.padding==1:\r\n            dec_inp = torch.ones([batch_y.shape[0], self.args.pred_len, batch_y.shape[-1]]).float()\r\n        dec_inp = torch.cat([batch_y[:,:self.args.label_len,:], dec_inp], dim=1).float().to(self.device)\r\n        # encoder - decoder\r\n        if self.args.use_amp:\r\n            with torch.cuda.amp.autocast():\r\n                if self.args.output_attention:\r\n                    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\r\n                else:\r\n                    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\r\n        else:\r\n            if self.args.output_attention:\r\n                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\r\n            else:\r\n                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\r\n        if self.args.inverse:\r\n            outputs = dataset_object.inverse_transform(outputs)\r\n        f_dim = -1 if self.args.features=='MS' else 0\r\n        batch_y = batch_y[:,-self.args.pred_len:,f_dim:].to(self.device)\r\n\r\n        return outputs, batch_y\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/exp/exp_informer.py b/exp/exp_informer.py
--- a/exp/exp_informer.py	(revision c95e0a73ba24d32854a065bfff23f8d414068de3)
+++ b/exp/exp_informer.py	(date 1683622238535)
@@ -18,6 +18,8 @@
 import warnings
 warnings.filterwarnings('ignore')
 
+from exp.focal_loss import BinaryFocalLoss
+
 class Exp_Informer(Exp_Basic):
     def __init__(self, args):
         super(Exp_Informer, self).__init__(args)
Index: exp/focal_loss.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/exp/focal_loss.py b/exp/focal_loss.py
new file mode 100644
--- /dev/null	(date 1683621063246)
+++ b/exp/focal_loss.py	(date 1683621063246)
@@ -0,0 +1,32 @@
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+class BinaryFocalLoss(nn.Module):
+    def __init__(self, alpha=1, gamma=2, reduction='mean', **kwargs):
+        super(BinaryFocalLoss, self).__init__()
+        self.alpha = alpha
+        self.gamma = gamma
+        self.smooth = 1e-6  # set '1e-4' when train with FP16
+        self.reduction = reduction
+
+        assert self.reduction in ['none', 'mean', 'sum']
+
+    def forward(self, output, target):
+        prob = torch.sigmoid(output)
+        prob = torch.clamp(prob, self.smooth, 1.0 - self.smooth)
+
+        target = target.unsqueeze(dim=1)
+        pos_mask = (target == 1).float()
+        neg_mask = (target == 0).float()
+
+        pos_weight = (pos_mask * torch.pow(1 - prob, self.gamma)).detach()
+        pos_loss = -pos_weight * torch.log(prob)  # / (torch.sum(pos_weight) + 1e-4)
+
+        neg_weight = (neg_mask * torch.pow(prob, self.gamma)).detach()
+        neg_loss = -self.alpha * neg_weight * F.logsigmoid(-output)  # / (torch.sum(neg_weight) + 1e-4)
+
+        loss = pos_loss + neg_loss
+        loss = loss.mean()
+        return loss
